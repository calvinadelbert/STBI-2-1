{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VTInYzNzsGwK"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# from plotly import graph_objs as go\n",
    "# import plotly.express as px\n",
    "# import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "# import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tN-m_Ot-sqTZ",
    "outputId": "72bf812e-748d-4d19-e6ba-148e8b64eb58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lusiana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FlsJh-rBsga9"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HXPpdyqQq26c",
    "outputId": "0f527c9d-8b63-451c-e5a7-72b0ca16daba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.8.2-py2.py3-none-any.whl (15.2 MB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.8.2 tenacity-8.0.1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "df = df.dropna(how=\"any\", axis=1)\n",
    "df.columns = ['target', 'message']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EJ_lgs77sWh_",
    "outputId": "02d42b26-d421-47c7-f3c9-93e58b7a35d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20\n",
       "1    ham                      Ok lar... Joking wif u oni...            6\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28\n",
       "3    ham  U dun say so early hor... U c already then say...           11\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_len'] = df['message'].apply(lambda x: len(x.split(' ')))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gSK0mmGasaeG",
    "outputId": "4a4337a9-c3db-4c51-8d6b-246f57939513"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in  a wkly comp to win fa cup final...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_clean'] = df['message'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RSm140JJsjqq",
    "outputId": "464828ce-6004-4416-927c-be21a57389d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entry  wkly comp win fa cup final tkts  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                              ok lar joking wif oni  \n",
       "2  free entry  wkly comp win fa cup final tkts  m...  \n",
       "3                      dun say early hor already say  \n",
       "4        nah dont think goes usf lives around though  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text\n",
    "    \n",
    "df['message_clean'] = df['message_clean'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q4UoD8MjsmLm"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yiDn5DbDsxEk",
    "outputId": "8c4fd3ac-1900-4a77-f776-6f03aed0154a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  \n",
       "0  go jurong point crazi avail bugi n great world...  \n",
       "1                                ok lar joke wif oni  \n",
       "2  free entri  wkli comp win fa cup final tkts  m...  \n",
       "3                      dun say earli hor alreadi say  \n",
       "4          nah dont think goe usf live around though  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_clean'] = df['message_clean'].apply(stemm_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T-BnGaH_szZT"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WCIXLRCps18l",
    "outputId": "04907652-ab0f-45c7-f84a-c80f0576418a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say ear hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  \n",
       "0  go jurong point crazi avail bugi n great world...  \n",
       "1                                ok lar joke wif oni  \n",
       "2  free entri  wkli comp win fa cup final tkts  m...  \n",
       "3                        dun say ear hor alreadi say  \n",
       "4          nah dont think goe usf live around though  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_clean'] = df['message_clean'].apply(preprocess_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9t9val2e7Vot",
    "outputId": "cbe2726c-677e-4dd9-e66a-861f45b5961f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say ear hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuh...</td>\n",
       "      <td>18</td>\n",
       "      <td>ok lor soni ericsson salesman ask shuhui say q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "      <td>5</td>\n",
       "      <td>ard  like dat lor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to ...</td>\n",
       "      <td>15</td>\n",
       "      <td>dont wait til least wednesday see get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "      <td>3</td>\n",
       "      <td>huh lei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "      <td>28</td>\n",
       "      <td>remind  get  pound free call credit detail gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5567 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                            message  message_len  \\\n",
       "0       ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1       ham                      Ok lar... Joking wif u oni...            6   \n",
       "2      spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3       ham  U dun say so early hor... U c already then say...           11   \n",
       "4       ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "...     ...                                                ...          ...   \n",
       "5562    ham  Ok lor... Sony ericsson salesman... I ask shuh...           18   \n",
       "5563    ham                                Ard 6 like dat lor.            5   \n",
       "5564    ham  Why don't you wait 'til at least wednesday to ...           15   \n",
       "5565    ham                                       Huh y lei...            3   \n",
       "5566   spam  REMINDER FROM O2: To get 2.50 pounds free call...           28   \n",
       "\n",
       "                                          message_clean  \n",
       "0     go jurong point crazi avail bugi n great world...  \n",
       "1                                   ok lar joke wif oni  \n",
       "2     free entri  wkli comp win fa cup final tkts  m...  \n",
       "3                           dun say ear hor alreadi say  \n",
       "4             nah dont think goe usf live around though  \n",
       "...                                                 ...  \n",
       "5562  ok lor soni ericsson salesman ask shuhui say q...  \n",
       "5563                                  ard  like dat lor  \n",
       "5564             dont wait til least wednesday see get   \n",
       "5565                                            huh lei  \n",
       "5566  remind  get  pound free call credit detail gre...  \n",
       "\n",
       "[5567 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wapmFyiXI6bO"
   },
   "outputs": [],
   "source": [
    "texts = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "12_JF9cZJqNm"
   },
   "outputs": [],
   "source": [
    "texts = df['message_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-KZ7OcdwZyW"
   },
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWz35u67RWuR",
    "outputId": "bdd16da2-a750-40f0-d235-758e4d81f012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tokenizer\n",
      "  Downloading tokenizer-3.4.1-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Tokenizer\n",
      "Successfully installed Tokenizer-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (3.18.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.8.0-py2.py3-none-any.whl (164 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lusiana\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0645a38655e4aff8b4a539d65badf32fbd4b92cf226f5aba54d53453aa999108\n",
      "  Stored in directory: c:\\users\\lusiana\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: rsa, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.8.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.3 keras-preprocessing-1.1.2 libclang-14.0.1 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "V0wFnAc1Q3w6"
   },
   "outputs": [],
   "source": [
    "# Calculate the length of our vocabulary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "word_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlnEo4OI-CxM",
    "outputId": "e7ec936a-7368-4cfb-c1cd-b58105a7de1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6726"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_bdxC8y602V",
    "outputId": "90a593ee-b490-4bff-cab6-0883fcef543e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lusiana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zHafSM7NQ3zR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YY2XCQWwx09J"
   },
   "outputs": [],
   "source": [
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    embed(texts), \n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "X = train_padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byRQijhH98wB",
    "outputId": "34c3869a-fb94-415f-b8da-e83dd876a3f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 3179,  274, ...,    0,    0,    0],\n",
       "       [   8,  236,  527, ...,    0,    0,    0],\n",
       "       [   9,  356,  588, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [6724, 1002, 6725, ...,    0,    0,    0],\n",
       "       [ 138, 1251, 1603, ...,    0,    0,    0],\n",
       "       [1986,  378,  170, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-3Yk-WvwRw-"
   },
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KZlduyZ0MJT1"
   },
   "outputs": [],
   "source": [
    "frame = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_cC7wL8FMF03"
   },
   "outputs": [],
   "source": [
    "doc_id = []\n",
    "for i in range(1, (len(frame['message_clean']))+1):\n",
    "  doc_id.append('D' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qPOUCQCJMF-c",
    "outputId": "b4e915a8-ea35-412f-aa62-8f9ed042afb4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "      <th>docID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say ear hor alreadi say</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "      <td>D5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean docID  \n",
       "0  go jurong point crazi avail bugi n great world...    D1  \n",
       "1                                ok lar joke wif oni    D2  \n",
       "2  free entri  wkli comp win fa cup final tkts  m...    D3  \n",
       "3                        dun say ear hor alreadi say    D4  \n",
       "4          nah dont think goe usf live around though    D5  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['docID'] = doc_id\n",
    "frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXb_Zw-zwNWV"
   },
   "source": [
    "# TFIDF with TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqwKP5TWGC8i"
   },
   "source": [
    "## Kode dibawah ini digunakan pada TFiDF Group\n",
    "Tidak digunakan pada TF-IDF non group karena membutuhkan waktu > 4 jam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Yc4pHUXJM0A1"
   },
   "outputs": [],
   "source": [
    "def term_document_matrix(data, vocab= None, document_index= 'ID', text= 'text'):\n",
    "    \"\"\"Calculate frequency of term in the document.\n",
    "    \n",
    "    parameter: \n",
    "        data: DataFrame. \n",
    "        Frequency of word calculated against the data.\n",
    "        \n",
    "        vocab: list of strings.\n",
    "        Vocabulary of the documents    \n",
    "        \n",
    "        document_index: str.\n",
    "        Column name for document index in DataFrame passed.\n",
    "        \n",
    "        text: str\n",
    "        Column name containing text for all documents in DataFrame,\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing term document matrix.\n",
    "        \"\"\"\n",
    "    \n",
    "    vocab_index = pd.DataFrame(columns=df[document_index], index= vocab).fillna(0)\n",
    "    \n",
    "    for word in vocab_index.index:\n",
    "        \n",
    "        for doc in data[document_index]:\n",
    "            \n",
    "            freq = data[data[document_index] == doc][text].values[0].count(word)\n",
    "            vocab_index.loc[word,doc] = freq\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "eOFP-a-SwCGq"
   },
   "outputs": [],
   "source": [
    "# menghitung jumlah kata di dalam dataset\n",
    "\n",
    "new_all_text = \" \".join(frame['message_clean'].apply(clean_text).values)\n",
    "vocab = []\n",
    "vocab = np.unique(word_tokenize(new_all_text))\n",
    "vocab = [word for word in vocab if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "SPi4itYwM2I_",
    "outputId": "b1b954d4-340a-4340-9535-655ca2f920c7"
   },
   "outputs": [],
   "source": [
    "# dapat dijalankan namun butuh waktu lama, sekitaran 4 jam untuk dataset ini\n",
    "\n",
    "similarity_index = ''\n",
    "similarity_index = term_document_matrix(frame,vocab,'docID','message_clean')\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY9UvlD_xeSr",
    "outputId": "f3a03cc2-62e8-458d-c366-139cab39ac89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa    42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_index.sum(axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNBMKpr9xBH4"
   },
   "outputs": [],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "uesHwl_MM04-"
   },
   "outputs": [],
   "source": [
    "# bergantung pada term_document_matrix\n",
    "def tf_idf_score(vocab_index, document_index, inv_df= 'inverse_document_frequency'):\n",
    "    \"\"\"\n",
    "    Calculate tf-idf score for vocabulary in documents\n",
    "    \n",
    "    parameter:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix.\n",
    "        \n",
    "        document_index: list or tuple.\n",
    "        Series containing document ids.\n",
    "        \n",
    "        inv_df: str.\n",
    "        Name of the column with calculated inverse document frequencies.\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing term document matrix and document frequencies, inverse document frequencies and tf-idf scores\n",
    "    \"\"\"\n",
    "    total_docx = len(document_index)\n",
    "    vocab_index['document_frequency'] = vocab_index.sum(axis= 1)\n",
    "    vocab_index['inverse_document_frequency'] = np.log2( total_docx / vocab_index['document_frequency'])\n",
    "    \n",
    "    for word in vocab_index.index:\n",
    "        \n",
    "        for doc in document_index:\n",
    "            \n",
    "                tf_idf = np.log2(1 + vocab_index.loc[word,doc]) * np.log2(vocab_index.loc[word][inv_df])\n",
    "                vocab_index.loc[word,'tf_idf_'+doc] = tf_idf\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "RIpELgJSNR0e",
    "outputId": "b2a39bd1-b30f-407f-bfe2-14302379a2a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-eb9000c9-295e-4370-b271-3a6cadb46d05\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>docID</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_D5563</th>\n",
       "      <th>tf_idf_D5564</th>\n",
       "      <th>tf_idf_D5565</th>\n",
       "      <th>tf_idf_D5566</th>\n",
       "      <th>tf_idf_D5567</th>\n",
       "      <th>tf_idf_D5568</th>\n",
       "      <th>tf_idf_D5569</th>\n",
       "      <th>tf_idf_D5570</th>\n",
       "      <th>tf_idf_D5571</th>\n",
       "      <th>tf_idf_D5572</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 11146 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb9000c9-295e-4370-b271-3a6cadb46d05')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-eb9000c9-295e-4370-b271-3a6cadb46d05 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-eb9000c9-295e-4370-b271-3a6cadb46d05');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "docID  D1  D2  D3  D4  D5  D6  D7  D8  D9  D10  ...  tf_idf_D5563  \\\n",
       "aa      0   0   0   0   0   0   0   0   0    0  ...           0.0   \n",
       "\n",
       "docID  tf_idf_D5564  tf_idf_D5565  tf_idf_D5566  tf_idf_D5567  tf_idf_D5568  \\\n",
       "aa              0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "docID  tf_idf_D5569  tf_idf_D5570  tf_idf_D5571  tf_idf_D5572  \n",
       "aa              0.0           0.0           0.0           0.0  \n",
       "\n",
       "[1 rows x 11146 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_index = tf_idf_score(similarity_index, df.docID.values)\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgCJiKrmN1n-"
   },
   "outputs": [],
   "source": [
    "similarity_index.to_csv('term_doc_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ7BNEHmN4Q9"
   },
   "outputs": [],
   "source": [
    "# test= pd.read_csv('term_doc_matrix.csv')\n",
    "# test = test.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUj5fdRlN5kw"
   },
   "outputs": [],
   "source": [
    "# def query_processing(query):\n",
    "#     \"\"\"\n",
    "#     Pre-processing query to accomodate calculations for tf-idf score\n",
    "    \n",
    "#     parameter:\n",
    "#         query: str.\n",
    "#         Textual query input to the system.\n",
    "        \n",
    "#     returns:\n",
    "#         query: str.\n",
    "#         Cleaned string.\n",
    "#         \"\"\"\n",
    "#     query= re.sub('\\W',' ',query)\n",
    "#     query= query.strip().lower()\n",
    "#     query= \" \".join([word for word in query.split() if word not in stopwords.words('english')])\n",
    "    \n",
    "#     return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3vrv_2H3Cri"
   },
   "outputs": [],
   "source": [
    "tfidfscoreingroup_a = tfidf_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2QKMob4Y1yxf"
   },
   "outputs": [],
   "source": [
    "tfidfscoreingroup_a.to_csv('term_doc_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3iXlMa610UV"
   },
   "outputs": [],
   "source": [
    "test= pd.read_csv('term_doc_matrix.csv')\n",
    "test = test.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnQcm-MgM8N6",
    "outputId": "8d5f6a20-b2d3-47ee-fea7-8b8d9f1f7622"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "4803           aa\n",
       "7033          aah\n",
       "9827          aah\n",
       "10895         aah\n",
       "12722       aaniy\n",
       "            ...  \n",
       "37445211       Ã»Ã²\n",
       "37445312       Ã»Ã²\n",
       "37445561       Ã»Ã²\n",
       "37448643       Ã»Ã²\n",
       "37453549    Ã»Ã³wel\n",
       "Name: variable, Length: 44239, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6MXciMGykH"
   },
   "outputs": [],
   "source": [
    "def query_score(vocab_index, query):\n",
    "    \"\"\"\n",
    "    Calculate tf-idf score for query terms\n",
    "    \n",
    "    parameter:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix with inverse document frequency and term frequencies calculated.\n",
    "        \n",
    "        query: str.\n",
    "        Query submitted to the system\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix with tf-idf scores for terms per document and query terms.\n",
    "    \"\"\"\n",
    "    for word in np.unique(query.split()):\n",
    "        \n",
    "        freq = query.count(word)\n",
    "        \n",
    "        if [vocab_index[variable].astype(str).str.contains(word, na=False) for variable in vocab_index]:\n",
    "            print('true')\n",
    "            tf_idf = np.log2(1+freq) * np.log2(vocab_index.loc[word].inverse_document_frequency)\n",
    "            import pdb\n",
    "            pdb()\n",
    "            vocab_index.loc[word,\"query_tf_idf\"] = tf_idf\n",
    "            vocab_index['query_tf_idf'].fillna(0, inplace=True)\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnan55lKOW1y",
    "outputId": "c8df8665-5b5c-4b29-85fb-4ee51ec384b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "# if [test[variable].astype(str).str.contains(\"aa\", na=False) for variable in test]:\n",
    "#   print('true')\n",
    "# else:\n",
    "#   print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ResYrQb7HsOZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train_set = frame.message_clean #Documents\n",
    "test_set = [\"aa aaooooright canteen woman\"] #Query\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vsV3m8AIRNC",
    "outputId": "514af29f-01e0-4594-c5a3-89e3e9d0d9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Vectorizer to train set\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Transform Vectorizer to test set\n",
      "[[1 0 0 ... 0 0 0]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0.5 0.  0.  ... 0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words = stopWords)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\n",
    "testVectorizerArray = vectorizer.transform(test_set).toarray()\n",
    "\n",
    "print('Fit Vectorizer to train set')\n",
    "print(trainVectorizerArray)\n",
    "print('Transform Vectorizer to test set')\n",
    "print(testVectorizerArray)\n",
    "\n",
    "transformer.fit(trainVectorizerArray)\n",
    "print(transformer.transform(trainVectorizerArray).toarray())\n",
    "\n",
    "transformer.fit(testVectorizerArray)\n",
    "\n",
    "tfidf = transformer.transform(testVectorizerArray)\n",
    "print(tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISyLZ_Dm53LE"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vocab_index, document_index, query_scores):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between the documents and query\n",
    "    \n",
    "    parameter:\n",
    "        \n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing tf-idf score per term for every document and for the query terms.\n",
    "        \n",
    "        document_index: list.\n",
    "        List of document ids.\n",
    "        \n",
    "        query_scores: str.\n",
    "        Column name in DataFrame containing query term tf-idf scores.\n",
    "        \n",
    "    returns:\n",
    "        cosine_scores: Series.\n",
    "        Cosine similarity scores of every document.\n",
    "    \"\"\"\n",
    "    cosine_scores = {}\n",
    "    \n",
    "    query_scalar = np.sqrt(sum(vocab_index[query_scores] ** 2))\n",
    "    \n",
    "    for doc in document_index:\n",
    "        \n",
    "        doc_scalar = np.sqrt(sum(vocab_index[doc] ** 2))\n",
    "        dot_prod = sum(vocab_index[doc] * vocab_index[query_scores])\n",
    "        cosine = (dot_prod / (query_scalar * doc_scalar))\n",
    "        \n",
    "        cosine_scores[doc] = cosine\n",
    "        \n",
    "    return pd.Series(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJXEXzXH6ANa"
   },
   "outputs": [],
   "source": [
    "cosines = cosine_similarity(similarity_index, df.docID.values, 'query_tf_idf')\n",
    "cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCSXlvLG8CYL"
   },
   "outputs": [],
   "source": [
    "def retrieve_index(data,cosine_scores, document_index):\n",
    "    \"\"\"\n",
    "    Retrieves indices for the corresponding document cosine scores\n",
    "    \n",
    "    parameters:\n",
    "        data: DataFrame.\n",
    "        DataFrame containing document ids and text.\n",
    "        \n",
    "        cosine_scores: Series.\n",
    "        Series containing document cosine scores.\n",
    "        \n",
    "        document_index: str.\n",
    "        Column name containing document ids in data.\n",
    "        \n",
    "    returns:\n",
    "        data: DataFrame.\n",
    "        Original DataFrame with cosine scores added as column.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.set_index(document_index)\n",
    "    data['scores'] = cosine_scores\n",
    "    \n",
    "    return data.reset_index().sort_values('scores',ascending=False).head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYV7hio084jH"
   },
   "outputs": [],
   "source": [
    "indices = retrieve_index(df, cosines, 'docID')\n",
    "indices"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF-DF SPAM DATASET WORD EMBED: GLOVE_STBI PROYEK.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
